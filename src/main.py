from __future__ import annotations

import time
from typing import Any, Dict

import httpx
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse

from .adapter import build_responses_request, to_chat_completions, to_completions
from .config import settings
from .logging_setup import configure_logging, get_logger
from .streaming import stream_chat_completions, stream_completions

configure_logging(settings.log_level)
logger = get_logger()

app = FastAPI(title="OpenAI Responses Adapter", version="0.1.0")


@app.get("/healthz")
async def healthz() -> Dict[str, str]:
    return {"status": "ok"}


async def _proxy(payload: Dict[str, Any], stream: bool, transform: str) -> Any:
    upstream_url = settings.upstream_base_url.rstrip("/") + settings.upstream_responses_path
    headers = {"Content-Type": "application/json"}
    headers.update(settings.auth_headers())

    timeout = httpx.Timeout(settings.request_timeout)

    async with httpx.AsyncClient(timeout=timeout) as client:
        start = time.time()
        try:
            if stream:
                response = await client.post(upstream_url, json=payload, headers=headers, stream=True)
            else:
                response = await client.post(upstream_url, json=payload, headers=headers)
        except httpx.RequestError as exc:
            logger.error("upstream.request_error", error=str(exc))
            return JSONResponse(status_code=502, content={"error": "upstream_unreachable"})

        elapsed_ms = int((time.time() - start) * 1000)
        logger.info(
            "upstream.response",
            status=response.status_code,
            elapsed_ms=elapsed_ms,
        )

        if stream:
            if response.status_code >= 400:
                data = await response.aread()
                return JSONResponse(status_code=response.status_code, content={"error": data.decode("utf-8", "ignore")})

            lines = response.aiter_lines()
            if transform == "chat":
                return StreamingResponse(stream_chat_completions(lines), media_type="text/event-stream")
            return StreamingResponse(stream_completions(lines), media_type="text/event-stream")

        if response.status_code >= 400:
            return JSONResponse(status_code=response.status_code, content={"error": response.text})

        body = response.json()
        if transform == "chat":
            return JSONResponse(content=to_chat_completions(body))
        return JSONResponse(content=to_completions(body))


@app.post("/v1/chat/completions")
async def chat_completions(request: Request) -> Any:
    payload = await request.json()
    model_map = settings.resolved_model_map()
    responses_payload = build_responses_request(payload, model_map)
    return await _proxy(responses_payload, bool(payload.get("stream")), "chat")


@app.post("/v1/completions")
async def completions(request: Request) -> Any:
    payload = await request.json()
    model_map = settings.resolved_model_map()
    responses_payload = build_responses_request(payload, model_map)
    return await _proxy(responses_payload, bool(payload.get("stream")), "completions")
